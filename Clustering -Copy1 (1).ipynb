{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35roXDEMudbw"
   },
   "source": [
    "# GUC Clustering Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCwbCzREudb1"
   },
   "source": [
    "Ahmad Ashraf Ahmad\n",
    "46-3060\n",
    "T-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIiItKbYudb2"
   },
   "source": [
    "**Objective:** \n",
    "The objective of this project teach students how to apply clustering to real data sets\n",
    "\n",
    "The projects aims to teach student: \n",
    "* Which clustering approach to use\n",
    "* Compare between Kmeans, Hierarchal, DBScan, and Gaussian Mixtures  \n",
    "* How to tune the parameters of each data approach\n",
    "* What is the effect of different distance functions (optional) \n",
    "* How to evaluate clustering approachs \n",
    "* How to display the output\n",
    "* What is the effect of normalizing the data \n",
    "\n",
    "Students in this project will use ready-made functions from Sklearn, plotnine, numpy and pandas \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtHElDYdudb3"
   },
   "outputs": [],
   "source": [
    "# if plotnine is not installed in Jupter then use the following command to install it \n",
    "!pip install plotnine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RHS5ZoQudb4"
   },
   "source": [
    "Running this project require the following imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrueqJenudb5"
   },
   "outputs": [],
   "source": [
    "pip install plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrueqJenudb5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "from sklearn.datasets import make_blobs\n",
    "from plotnine import *   \n",
    "# StandardScaler is a function to normalize the data \n",
    "# You may also check MinMaxScaler and MaxAbsScaler \n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ju2Zj6-nudb5"
   },
   "outputs": [],
   "source": [
    "# helper function that allows us to display data in 2 dimensions an highlights the clusters\n",
    "def display_cluster(X,km=[],num_clusters=0):\n",
    "    color = 'brgcmyk'  #List colors\n",
    "    alpha = 0.5  #color obaque\n",
    "    s = 20\n",
    "    if num_clusters == 0:\n",
    "        plt.scatter(X[:,0],X[:,1],c = color[0],alpha = alpha,s = s)\n",
    "    else:\n",
    "        for i in range(num_clusters):\n",
    "            plt.scatter(X[km.labels_==i,0],X[km.labels_==i,1],c = color[i],alpha = alpha,s=s)\n",
    "            plt.scatter(km.cluster_centers_[i][0],km.cluster_centers_[i][1],c = color[i], marker = 'x', s = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ju2Zj6-nudb5"
   },
   "outputs": [],
   "source": [
    "# helper function that allows us to display data in 2 dimensions an highlights the clusters\n",
    "def display_cluster_h(X,km=[],num_clusters=0):\n",
    "    color = 'brgcmyk'  #List colors\n",
    "    alpha = 0.5  #color obaque\n",
    "    s = 20\n",
    "    if num_clusters == 0:\n",
    "        plt.scatter(X[:,0],X[:,1],c = color[0],alpha = alpha,s = s)\n",
    "    else:\n",
    "        for i in range(num_clusters):\n",
    "            plt.scatter(X[km.labels_==i,0],X[km.labels_==i,1],cmap = 'plasma',alpha = alpha,s=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZnIbT3Mudb6"
   },
   "source": [
    "## Multi Blob Data Set \n",
    "* The Data Set generated below has 6 cluster with varying number of users and varing densities\n",
    "* Cluster the data set below using \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeSqG318udb7",
    "outputId": "078fad92-3073-4558-b1e8-f0acd8d85d34"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "n_bins = 6  \n",
    "centers = [(-3, -3), (0, 0), (5,2.5),(-1, 4), (4, 6), (9,7)]\n",
    "Multi_blob_Data, y = make_blobs(n_samples=[100,150, 300, 400,300, 200], n_features=2, cluster_std=[1.3,0.6, 1.2, 1.7,0.9,1.7],\n",
    "                  centers=centers, shuffle=False, random_state=42)\n",
    "display_cluster(Multi_blob_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDSIGjubudb8"
   },
   "source": [
    "### Kmeans \n",
    "* Use Kmeans with different values of K to cluster the above data \n",
    "* Display the outcome of each value of K \n",
    "* Plot distortion function versus K and choose the approriate value of k \n",
    "* Plot the silhouette_score versus K and use it to choose the best K \n",
    "* Store the silhouette_score for the best K for later comparison with other clustering techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "source": [
    "#### Kmeans with different K values\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(Multi_blob_Data)\n",
    "plt.figure(figsize=(50, 25))\n",
    "for i in range(1,15):\n",
    "    kmeans = KMeans(n_clusters=i,random_state=42)\n",
    "    kmeans.fit(Multi_blob_Data)\n",
    "    plt.subplot(3, 5, i)\n",
    "    plt.scatter(x=df[0],y=df[1],c=kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "source": [
    "##### Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "outputs": [],
   "source": [
    "inertias = []\n",
    "\n",
    "for i in range(1,15):\n",
    "    kmeans = KMeans(n_clusters=i,random_state=42)\n",
    "    kmeans.fit(Multi_blob_Data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,15), inertias, marker='o')\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "source": [
    "##### K=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "source": [
    "##### Silhouette Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(2,15):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit_predict(Multi_blob_Data)\n",
    "    scores.append(silhouette_score(Multi_blob_Data, kmeans.labels_, metric='euclidean'))\n",
    "\n",
    "plt.plot(range(2,15), scores, marker='o')\n",
    "plt.title('Silhouette method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "source": [
    "##### K=6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ne3KmtPudb9"
   },
   "source": [
    "Documentation: \n",
    "metricstr or callable, default=’euclidean’\n",
    "The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by metrics.pairwise.pairwise_distances. If X is the distance array itself, use metric=\"precomputed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE7dvpOAudb9"
   },
   "source": [
    "### Hierarchal Clustering\n",
    "* Use AgglomerativeClustering function to  to cluster the above data \n",
    "* In the  AgglomerativeClustering change the following parameters \n",
    "    * Affinity (use euclidean, manhattan and cosine)\n",
    "    * Linkage( use average and single )\n",
    "    * Distance_threshold (try different)\n",
    "* For each of these trials plot the Dendograph , calculate the silhouette_score and display the resulting clusters  \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3O_6WwKoudb-"
   },
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3O_6WwKoudb-"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "df=pd.DataFrame(Multi_blob_Data)\n",
    "plt.figure(figsize=(100, 100))\n",
    "scores=[]\n",
    "clusters=[]\n",
    "parameters=[]\n",
    "linkage_array=[\"average\",\"single\"]\n",
    "affinity_array=[\"euclidean\",\"manhattan\",\"cosine\"]\n",
    "z=1\n",
    "for k in range(1,3):\n",
    "    link=linkage_array[k-1]\n",
    "    for l in range (1,4):\n",
    "        a=affinity_array[l-1]\n",
    "        if link==\"average\" and a==\"cosine\":\n",
    "            start=0.3\n",
    "            stop=0.8\n",
    "            step=0.1\n",
    "        elif link==\"single\" and a==\"cosine\":\n",
    "            start=0.05\n",
    "            stop=0.1\n",
    "            step=0.01            \n",
    "        elif link==\"single\" and (a==\"manhattan\" or a==\"euclidean\"):\n",
    "            start=0.05\n",
    "            stop=0.1\n",
    "            step=0.01\n",
    "        else:\n",
    "            start=1\n",
    "            stop=6\n",
    "            step=1\n",
    "        for m in np.arange (start,stop,step):\n",
    "            d=m\n",
    "            clt = AgglomerativeClustering(linkage=link, affinity=a,distance_threshold=d,n_clusters=None)\n",
    "            model = clt.fit(Multi_blob_Data)\n",
    "            if (len(np.unique(model.labels_)))>1:\n",
    "                plt.subplot(11, 5,z)\n",
    "                plot_dendrogram(model, truncate_mode=\"level\", p=8)\n",
    "                plt.title(str(link)+\" , \"+str(a)+\" , \"+str(d))\n",
    "                plt.subplot(11, 5,z+1)\n",
    "                plt.scatter(x=df[0],y=df[1],c=model.labels_)\n",
    "                plt.title(str(link)+\" , \"+str(a)+\" , \"+str(d))\n",
    "                scores.append(silhouette_score(df[[0,1]],model.labels_ ))\n",
    "                clusters.append(len(np.unique(model.labels_)))\n",
    "                parameters.append(str(link)+\" , \"+str(a)+\" , \"+str(d))\n",
    "                z=z+2\n",
    "plt.subplot(11,5,z)\n",
    "plt.plot(clusters, scores, marker='o')\n",
    "plt.title('Silhouette method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Score')\n",
    "# print( \"number of clusters= \" + str(clusters[scores.index(max(scores))])+\n",
    "#       \" and parameters \"+str(parameters[scores.index(max(scores))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myJE7vQKudb-"
   },
   "source": [
    "### DBScan\n",
    "* Use DBScan function to  to cluster the above data \n",
    "* In the  DBscan change the following parameters \n",
    "    * EPS (from 0.1 to 3)\n",
    "    * Min_samples (from 5 to 25)\n",
    "* Plot the silhouette_score versus the variation in the EPS and the min_samples\n",
    "* Plot the resulting Clusters in this case \n",
    "* Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques. \n",
    "* Record your observations and comments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiQtpAt5udb_"
   },
   "source": [
    "Silhouette Method: This technique measures the separability between clusters. First, an average distance is found between each point and all other points in a cluster. Then it measures the distance between each point and each point in other clusters. We subtract the two average measures and divide by whichever average is larger.\n",
    "We ultimately want a high (ie. closest to 1) score which would indicate that there is a small intra-cluster average distance (tight clusters) and a large inter-cluster average distance (clusters well separated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiQtpAt5udb_"
   },
   "outputs": [],
   "source": [
    "scores=[]\n",
    "clusters=[]\n",
    "parameters=[]\n",
    "EPSS=[]\n",
    "Min_Sampless=[]\n",
    "df=pd.DataFrame(Multi_blob_Data)\n",
    "z=1\n",
    "plt.figure(figsize=(100, 100))\n",
    "for Min_Samples in range (5,26):\n",
    "    for EPS in np.arange (0.1,4,0.3):\n",
    "        model = (DBSCAN(eps=EPS, min_samples=Min_Samples)).fit(Multi_blob_Data)\n",
    "        if (len(np.unique(model.labels_)))>1:\n",
    "            plt.subplot(17, 10,z)\n",
    "            plt.scatter(x=df[0],y=df[1],c=model.labels_)\n",
    "            plt.title(\"EPS=\"+str(EPS)+\" Min_Samples= \"+str(Min_Samples))\n",
    "            scores.append(silhouette_score(df,model.labels_ ))\n",
    "            clusters.append(len(np.unique(model.labels_)))\n",
    "            EPSS.append(EPS)\n",
    "            Min_Sampless.append(Min_Samples)\n",
    "            parameters.append(str(EPS)+\" , \"+str(Min_Samples))\n",
    "            z=z+1\n",
    "plt.subplot(17,10,z)\n",
    "plt.plot(EPSS, scores, marker='o')\n",
    "plt.title(\"EPS Variation with Silhouette Score \")\n",
    "plt.xlabel('EPS')\n",
    "plt.ylabel('Score')\n",
    "plt.subplot(17,10,z+1)\n",
    "plt.plot(Min_Sampless, scores, marker='o')\n",
    "plt.title(\"Min_points Variation with Silhouette Score \")\n",
    "plt.xlabel('Min_points')\n",
    "plt.ylabel('Score')\n",
    "print( \"number of clusters= \" + str(clusters[scores.index(max(scores))])+\n",
    "      \" and parameters \"+str(parameters[scores.index(max(scores))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ip16g1QFudb_"
   },
   "outputs": [],
   "source": [
    "maximum_ss=max(scores)\n",
    "maximum_index=scores.index(maximum_ss)\n",
    "best_eps=EPSS[maximum_index]\n",
    "best_MinPoints=Min_Sampless[maximum_index]\n",
    "print(\"Best EPS=\",best_eps,\"and best MinPoints=\",best_MinPoints,\"for the best silhoutte score of\",maximum_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip16g1QFudb_"
   },
   "source": [
    "### Gaussian Mixture\n",
    "* Use GaussianMixture function to cluster the above data \n",
    "* In GMM change the covariance_type and check the difference in the resulting proabability fit \n",
    "* Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ip16g1QFudb_"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "covariance = ['spherical', 'diag', 'tied', 'full']\n",
    "p=0\n",
    "plt.figure(figsize=(50, 25))\n",
    "for x in covariance:\n",
    "    # Initialize the GMM model.\n",
    "    GMM_Blob = GaussianMixture(n_components=2, covariance_type=x, random_state=0).fit(Multi_blob_Data)\n",
    "    labels = GMM_Blob.predict(Multi_blob_Data)\n",
    "    \n",
    "    # Define a grid to plot the contour plot\n",
    "    x_min, x_max = Multi_blob_Data[:, 0].min() - 1, Multi_blob_Data[:, 0].max() + 1\n",
    "    y_min, y_max = Multi_blob_Data[:, 1].min() - 1, Multi_blob_Data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = np.zeros((xx.shape[0], xx.shape[1]))\n",
    "    p=p+1\n",
    "    for i in range(GMM_Blob.n_components):\n",
    "        density = multivariate_normal(mean=GMM_Blob.means_[i], cov=GMM_Blob.covariances_[i]).pdf(np.dstack((xx, yy)))\n",
    "        Z += density * GMM_Blob.weights_[i]\n",
    "    \n",
    "    # Plot the resulting contour plot\n",
    "    plt.subplot(2,2,p)\n",
    "    plt.contour(xx, yy, Z, levels=10, linewidths=1, colors='black')\n",
    "    plt.contourf(xx, yy, Z, levels=10, cmap='tab20b')\n",
    "    plt.scatter(Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=GMM_Blob.predict(Multi_blob_Data), cmap='tab20b')\n",
    "    plt.title('Gaussian Mixture Clustering using %s' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m92lZkkyudb_"
   },
   "source": [
    "## iris data set \n",
    "The iris data set is test data set that is part of the Sklearn module \n",
    "which contains 150 records each with 4 features. All the features are represented by real numbers \n",
    "\n",
    "The data represents three classes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyoCVfyMudcA"
   },
   "source": [
    "* Repeat all the above clustering approaches and steps on the above data \n",
    "* Normalize the data then repeat all the above steps \n",
    "* Compare between the different clustering approaches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QaCWyyCudcA",
    "outputId": "79c14dba-80cf-4d96-e69d-70763b789faf"
   },
   "source": [
    "## Before Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QaCWyyCudcA",
    "outputId": "79c14dba-80cf-4d96-e69d-70763b789faf"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "iris_data.target[[10, 25, 50]]\n",
    "#array([0, 0, 1])\n",
    "# list(iris_data.target_names)\n",
    "# ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "# df = pd.DataFrame(iris_data, columns = ['Sepal Length','Sepal Width','Petal Length','Petal Width']) \n",
    "# df\n",
    "df = pd.DataFrame(data= np.c_[iris_data['data']],\n",
    "                     columns= iris_data['feature_names'] )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    for j in range(0,4):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i])+\" and \"+ str(df.columns[j])\n",
    "            plt.figure(figsize=(50, 25))\n",
    "            plt.title(Features_string)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        inertias = []\n",
    "        scores = []\n",
    "        clusters_string=[]\n",
    "        clusters=[]\n",
    "        for num_clusters in range(2,18):\n",
    "            kmeans = KMeans(n_clusters=num_clusters,random_state=42)\n",
    "            kmeans.fit(df[[df.columns[i],df.columns[j]]])\n",
    "            inertias.append(kmeans.inertia_)\n",
    "            scores.append(silhouette_score(df[[df.columns[i],df.columns[j]]], kmeans.labels_, metric='euclidean'))\n",
    "            clusters_string=str(df.columns[i])+\" and \"+str(df.columns[j])+\" for \"+str(num_clusters)+\" Clusters \"\n",
    "            clusters.append(len(np.unique(kmeans.labels_)))\n",
    "            plt.subplot(5, 4, num_clusters-1)\n",
    "            plt.scatter(x=df[df.columns[i]],y=df[df.columns[j]],c=kmeans.labels_)\n",
    "            plt.title(clusters_string)\n",
    "        plt.subplot(5,4,num_clusters)\n",
    "        plt.plot(range(2,num_clusters+1), inertias, marker='o')\n",
    "        plt.title('Elbow method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.subplot(5,4,num_clusters+1)\n",
    "        plt.plot(range(2,num_clusters+1), scores, marker='o')\n",
    "        plt.title('Silhouette method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Score')\n",
    "        print(\"Based on silhouette scores, the best number of clusters for \"+ Features_string +\" is \"+str(clusters[scores.index(max(scores))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "scores=[]\n",
    "clusters=[]\n",
    "parameters=[]\n",
    "for i in range(0,4):\n",
    "    for j in range(0,4):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i])+\" and \"+ str(df.columns[j])\n",
    "            plt.figure(figsize=(100, 50))\n",
    "        else:\n",
    "            continue\n",
    "        scores=[]\n",
    "        clusters=[]\n",
    "        parameters=[]\n",
    "        linkage_array=[\"average\",\"single\"]\n",
    "        affinity_array=[\"euclidean\",\"manhattan\",\"cosine\"]\n",
    "        z=1\n",
    "        for k in range(1,3):\n",
    "            link=linkage_array[k-1]\n",
    "            for l in range (1,4):\n",
    "                a=affinity_array[l-1]\n",
    "                if link==\"average\" and a==\"cosine\":\n",
    "                    start=0.3\n",
    "                    stop=0.8\n",
    "                    step=0.1\n",
    "                elif link==\"single\" and a==\"cosine\":\n",
    "                    start=0.05\n",
    "                    stop=0.1\n",
    "                    step=0.01            \n",
    "                elif link==\"single\" and (a==\"manhattan\" or a==\"euclidean\"):\n",
    "                    start=0.05\n",
    "                    stop=0.1\n",
    "                    step=0.01\n",
    "                else:\n",
    "                    start=1\n",
    "                    stop=6\n",
    "                    step=1\n",
    "                for m in np.arange (start,stop,step):\n",
    "                    d=m\n",
    "                    clt = AgglomerativeClustering(linkage=link, affinity=a,distance_threshold=d,n_clusters=None)\n",
    "                    model = clt.fit(df[[df.columns[i],df.columns[j]]])\n",
    "                    if (len(np.unique(model.labels_)))>1:\n",
    "                        plt.subplot(13, 5,z)\n",
    "                        plot_dendrogram(model, truncate_mode=\"level\", p=8)\n",
    "                        plt.title(str(link)+\" , \"+str(a)+\" , \"+str(d)+\" for \"+ Features_string)\n",
    "                        plt.subplot(13, 5,z+1)\n",
    "                        plt.scatter(x=df[df.columns[i]],y=df[df.columns[j]],c=model.labels_)\n",
    "                        plt.title(str(link)+\" , \"+str(a)+\" , \"+str(d)+\" for \"+ Features_string)\n",
    "                        scores.append(silhouette_score(df[[df.columns[i],df.columns[j]]],model.labels_ ))\n",
    "                        clusters.append(len(np.unique(model.labels_)))\n",
    "                        parameters.append(str(link)+\" , \"+str(a)+\" , \"+str(d)+\" for \"+ Features_string)\n",
    "                        z=z+2\n",
    "        plt.subplot(13,5,z)\n",
    "        plt.plot(clusters, scores, marker='o')\n",
    "        plt.title('Silhouette method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Score')\n",
    "        print( \"number of clusters= \" + str(clusters[scores.index(max(scores))])+\n",
    "              \" and parameters \"+str(parameters[scores.index(max(scores))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "clusters=[]\n",
    "parameters=[]\n",
    "EPSS=[]\n",
    "Min_Sampless=[]\n",
    "for i in range(0,4):\n",
    "    for j in range(0,4):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i])+\" and \"+ str(df.columns[j])\n",
    "            plt.figure(figsize=(100, 50))\n",
    "        else:\n",
    "            continue\n",
    "        scores=[]\n",
    "        clusters=[]\n",
    "        parameters=[]\n",
    "        EPSS=[]\n",
    "        Min_Sampless=[]\n",
    "        z=1\n",
    "        for Min_Samples in range (5,26):\n",
    "            for EPS in np.arange (0.1,4,0.3):\n",
    "                model = (DBSCAN(eps=EPS, min_samples=Min_Samples)).fit(df[[df.columns[i],df.columns[j]]])\n",
    "                if (len(np.unique(model.labels_)))>1:\n",
    "                    plt.subplot(21, 10,z)\n",
    "                    plt.scatter(x=df[df.columns[i]],y=df[df.columns[j]],c=model.labels_)\n",
    "                    plt.title(str(EPS)+\" , \"+str(Min_Samples)+\" for \"+ Features_string)\n",
    "                    scores.append(silhouette_score(df[[df.columns[i],df.columns[j]]],model.labels_ ))\n",
    "                    clusters.append(len(np.unique(model.labels_)))\n",
    "                    EPSS.append(EPS)\n",
    "                    Min_Sampless.append(Min_Samples)\n",
    "                    parameters.append(str(EPS)+\" , \"+str(Min_Samples)+\" for \"+ Features_string)\n",
    "                    z=z+1\n",
    "        plt.subplot(21,10,z)\n",
    "        plt.plot(EPSS, scores, marker='o')\n",
    "        plt.title(\"EPS Variation with Silhouette Score \")\n",
    "        plt.xlabel('EPS')\n",
    "        plt.ylabel('Score')\n",
    "        plt.subplot(21,10,z+1)\n",
    "        plt.plot(Min_Sampless, scores, marker='o')\n",
    "        plt.title(\"Min_points Variation with Silhouette Score \")\n",
    "        plt.xlabel('Min_points')\n",
    "        plt.ylabel('Score')\n",
    "        print( \"number of clusters= \" + str(clusters[scores.index(max(scores))])+\n",
    "              \" and parameters \"+str(parameters[scores.index(max(scores))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "for i in range(0,4):\n",
    "    for j in range(0,4):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i])+\" and \"+ str(df.columns[j])\n",
    "            plt.figure(figsize=(100, 50))\n",
    "        else:\n",
    "            continue \n",
    "        scores=[]\n",
    "        clusters=[]\n",
    "        parameters=[]\n",
    "        EPSS=[]\n",
    "        Min_Sampless=[]\n",
    "        data=(df[[df.columns[i],df.columns[j]]]).to_numpy()\n",
    "        covariance_type=[\"full\", \"tied\", \"diag\", \"spherical\"]\n",
    "        for k in  range(0,4):\n",
    "            gm = GaussianMixture(n_components=2,covariance_type=covariance_type[k]).fit(data)\n",
    "            x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "            y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "            xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "            Z = np.zeros((xx.shape[0], xx.shape[1]))\n",
    "            for l in range(gm.n_components):\n",
    "                density = multivariate_normal(mean=gm.means_[l], cov=gm.covariances_[l]).pdf(np.dstack((xx, yy)))\n",
    "                Z += density * gm.weights_[l]\n",
    "\n",
    "            # Plot the resulting contour plot\n",
    "            plt.subplot(2,2,k+1)\n",
    "            plt.contour(xx, yy, Z, levels=10, linewidths=1, colors='black')\n",
    "            plt.contourf(xx, yy, Z, levels=10, cmap='tab20b')\n",
    "            plt.scatter(data[:, 0], data[:, 1], c=gm.predict(data), cmap='tab20b')\n",
    "            plt.title('Gaussian Mixture Clustering using %s' % covariance_type[k]+ \" for \"+Features_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "data=scaler.fit_transform(df)\n",
    "df = pd.DataFrame(data,\n",
    "                     columns= iris_data['feature_names'] )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,4):\n",
    "    for j in range(0,4):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i])+\" and \"+ str(df.columns[j])\n",
    "            plt.figure(figsize=(50, 25))\n",
    "            plt.title(Features_string)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        inertias = []\n",
    "        scores = []\n",
    "        clusters_string=[]\n",
    "        clusters=[]\n",
    "        for num_clusters in range(2,18):\n",
    "            kmeans = KMeans(n_clusters=num_clusters,random_state=42)\n",
    "            kmeans.fit(df[[df.columns[i],df.columns[j]]])\n",
    "            inertias.append(kmeans.inertia_)\n",
    "            scores.append(silhouette_score(df[[df.columns[i],df.columns[j]]], kmeans.labels_, metric='euclidean'))\n",
    "            clusters_string=str(df.columns[i])+\" and \"+str(df.columns[j])+\" for \"+str(num_clusters)+\" Clusters \"\n",
    "            clusters.append(len(np.unique(kmeans.labels_)))\n",
    "            plt.subplot(5, 4, num_clusters-1)\n",
    "            plt.scatter(x=df[df.columns[i]],y=df[df.columns[j]],c=kmeans.labels_)\n",
    "            plt.title(clusters_string)\n",
    "        plt.subplot(5,4,num_clusters)\n",
    "        plt.plot(range(2,num_clusters+1), inertias, marker='o')\n",
    "        plt.title('Elbow method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.subplot(5,4,num_clusters+1)\n",
    "        plt.plot(range(2,num_clusters+1), scores, marker='o')\n",
    "        plt.title('Silhouette method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Score')\n",
    "        print(\"Based on silhouette scores, the best number of clusters for \"+ Features_string +\" is \"+str(clusters[scores.index(max(scores))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "scores=[]\n",
    "clusters=[]\n",
    "parameters=[]\n",
    "for i in range(0,4):\n",
    "    for j in range(0,4):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i])+\" and \"+ str(df.columns[j])\n",
    "            plt.figure(figsize=(100, 50))\n",
    "        else:\n",
    "            continue\n",
    "        scores=[]\n",
    "        clusters=[]\n",
    "        parameters=[]\n",
    "        linkage_array=[\"average\",\"single\"]\n",
    "        affinity_array=[\"euclidean\",\"manhattan\",\"cosine\"]\n",
    "        z=1\n",
    "        for k in range(1,3):\n",
    "            link=linkage_array[k-1]\n",
    "            for l in range (1,4):\n",
    "                a=affinity_array[l-1]\n",
    "                if link==\"average\" and a==\"cosine\":\n",
    "                    start=0.3\n",
    "                    stop=0.8\n",
    "                    step=0.1\n",
    "                elif link==\"single\" and a==\"cosine\":\n",
    "                    start=0.05\n",
    "                    stop=0.1\n",
    "                    step=0.01            \n",
    "                elif link==\"single\" and (a==\"manhattan\" or a==\"euclidean\"):\n",
    "                    start=0.05\n",
    "                    stop=0.1\n",
    "                    step=0.01\n",
    "                else:\n",
    "                    start=1\n",
    "                    stop=6\n",
    "                    step=1\n",
    "                for m in np.arange (start,stop,step):\n",
    "                    d=m\n",
    "                    clt = AgglomerativeClustering(linkage=link, affinity=a,distance_threshold=d,n_clusters=None)\n",
    "                    model = clt.fit(df[[df.columns[i],df.columns[j]]])\n",
    "                    if (len(np.unique(model.labels_)))>1:\n",
    "                        plt.subplot(13, 5,z)\n",
    "                        plot_dendrogram(model, truncate_mode=\"level\", p=8)\n",
    "                        plt.title(str(link)+\" , \"+str(a)+\" , \"+str(d)+\" for \"+ Features_string)\n",
    "                        plt.subplot(13, 5,z+1)\n",
    "                        plt.scatter(x=df[df.columns[i]],y=df[df.columns[j]],c=model.labels_)\n",
    "                        plt.title(str(link)+\" , \"+str(a)+\" , \"+str(d)+\" for \"+ Features_string)\n",
    "                        scores.append(silhouette_score(df[[df.columns[i],df.columns[j]]],model.labels_ ))\n",
    "                        clusters.append(len(np.unique(model.labels_)))\n",
    "                        parameters.append(str(link)+\" , \"+str(a)+\" , \"+str(d)+\" for \"+ Features_string)\n",
    "                        z=z+2\n",
    "        plt.subplot(13,5,z)\n",
    "        plt.plot(clusters, scores, marker='o')\n",
    "        plt.title('Silhouette method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Score')\n",
    "        print( \"number of clusters= \" + str(clusters[scores.index(max(scores))])+\n",
    "              \" and parameters \"+str(parameters[scores.index(max(scores))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "clusters=[]\n",
    "parameters=[]\n",
    "EPSS=[]\n",
    "Min_Sampless=[]\n",
    "for i in range(0,4):\n",
    "    for j in range(0,4):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i])+\" and \"+ str(df.columns[j])\n",
    "            plt.figure(figsize=(100, 50))\n",
    "        else:\n",
    "            continue\n",
    "        scores=[]\n",
    "        clusters=[]\n",
    "        parameters=[]\n",
    "        EPSS=[]\n",
    "        Min_Sampless=[]\n",
    "        z=1\n",
    "        for Min_Samples in range (5,26):\n",
    "            for EPS in np.arange (0.1,4,0.3):\n",
    "                model = (DBSCAN(eps=EPS, min_samples=Min_Samples)).fit(df[[df.columns[i],df.columns[j]]])\n",
    "                if (len(np.unique(model.labels_)))>1:\n",
    "                    plt.subplot(21, 10,z)\n",
    "                    plt.scatter(x=df[df.columns[i]],y=df[df.columns[j]],c=model.labels_)\n",
    "                    plt.title(str(EPS)+\" , \"+str(Min_Samples)+\" for \"+ Features_string)\n",
    "                    scores.append(silhouette_score(df[[df.columns[i],df.columns[j]]],model.labels_ ))\n",
    "                    clusters.append(len(np.unique(model.labels_)))\n",
    "                    EPSS.append(EPS)\n",
    "                    Min_Sampless.append(Min_Samples)\n",
    "                    parameters.append(str(EPS)+\" , \"+str(Min_Samples)+\" for \"+ Features_string)\n",
    "                    z=z+1\n",
    "        plt.subplot(21,10,z)\n",
    "        plt.plot(EPSS, scores, marker='o')\n",
    "        plt.title(\"EPS Variation with Silhouette Score \")\n",
    "        plt.xlabel('EPS')\n",
    "        plt.ylabel('Score')\n",
    "        plt.subplot(21,10,z+1)\n",
    "        plt.plot(Min_Sampless, scores, marker='o')\n",
    "        plt.title(\"Min_points Variation with Silhouette Score \")\n",
    "        plt.xlabel('Min_points')\n",
    "        plt.ylabel('Score')\n",
    "        print( \"number of clusters= \" + str(clusters[scores.index(max(scores))])+\n",
    "              \" and parameters \"+str(parameters[scores.index(max(scores))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "for i in range(0,4):\n",
    "    for j in range(0,4):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i])+\" and \"+ str(df.columns[j])\n",
    "            plt.figure(figsize=(100, 50))\n",
    "        else:\n",
    "            continue \n",
    "        scores=[]\n",
    "        clusters=[]\n",
    "        parameters=[]\n",
    "        EPSS=[]\n",
    "        Min_Sampless=[]\n",
    "        data=(df[[df.columns[i],df.columns[j]]]).to_numpy()\n",
    "        covariance_type=[\"full\", \"tied\", \"diag\", \"spherical\"]\n",
    "        for k in  range(0,4):\n",
    "            gm = GaussianMixture(n_components=2,covariance_type=covariance_type[k]).fit(data)\n",
    "            x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "            y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "            xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "            Z = np.zeros((xx.shape[0], xx.shape[1]))\n",
    "            for l in range(gm.n_components):\n",
    "                density = multivariate_normal(mean=gm.means_[l], cov=gm.covariances_[l]).pdf(np.dstack((xx, yy)))\n",
    "                Z += density * gm.weights_[l]\n",
    "\n",
    "            # Plot the resulting contour plot\n",
    "            plt.subplot(2,2,k+1)\n",
    "            plt.contour(xx, yy, Z, levels=10, linewidths=1, colors='black')\n",
    "            plt.contourf(xx, yy, Z, levels=10, cmap='tab20b')\n",
    "            plt.scatter(data[:, 0], data[:, 1], c=gm.predict(data), cmap='tab20b')\n",
    "            plt.title('Gaussian Mixture Clustering using %s' % covariance_type[k]+ \" for \"+Features_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized data shows better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2oBmWT2udcA"
   },
   "source": [
    "## Customer dataset\n",
    "Data preparation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Customer data.csv',index_col='ID')\n",
    "df.dropna(inplace=True)\n",
    "#remove the first column as it is unique to each customer and won't be useful for clustering \n",
    "#normailize\n",
    "scaler = preprocessing.StandardScaler()\n",
    "df[['Sex_n','Martial status_n','Age_n','Education_n','Income_n','Ocupation_n','Sttlement size_n']]=scaler.fit_transform(df[df.columns])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(7,14):\n",
    "    for j in range(7,14):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i-7])+\" and \"+str(df.columns[j-7])\n",
    "            plt.figure(figsize=(50, 25))\n",
    "            plt.title(Features_string)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        inertias = []\n",
    "        scores = []\n",
    "        clusters_string=[]\n",
    "        clusters=[]\n",
    "        for num_clusters in range(2,18):\n",
    "            kmeans = KMeans(n_clusters=num_clusters,random_state=42)\n",
    "            kmeans.fit(df[[df.columns[i],df.columns[j]]])\n",
    "            inertias.append(kmeans.inertia_)\n",
    "            scores.append(silhouette_score(df[[df.columns[i],df.columns[j]]], kmeans.labels_, metric='euclidean'))\n",
    "            clusters_string=str(df.columns[i])+\" and \"+str(df.columns[j])+\" for \"+str(num_clusters)+\" Clusters \"\n",
    "            clusters.append(len(np.unique(kmeans.labels_)))\n",
    "            plt.subplot(5, 4, num_clusters-1)\n",
    "            plt.scatter(x=df[df.columns[i]],y=df[df.columns[j]],c=kmeans.labels_)\n",
    "            plt.title(clusters_string)\n",
    "        plt.subplot(5,4,num_clusters)\n",
    "        plt.plot(range(2,num_clusters+1), inertias, marker='o')\n",
    "        plt.title('Elbow method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Inertia')\n",
    "        plt.subplot(5,4,num_clusters+1)\n",
    "        plt.plot(range(2,num_clusters+1), scores, marker='o')\n",
    "        plt.title('Silhouette method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Score')\n",
    "        print(\"Based on silhouette scores, the best number of clusters for \"+ Features_string +\" is \"+str(clusters[scores.index(max(scores))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In order to get  the best number of clusters for each features combination, use the silhouette score (most silhouette score) and elbow methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchal Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "scores=[]\n",
    "clusters=[]\n",
    "parameters=[]\n",
    "for i in range(7,14):\n",
    "    for j in range(7,14):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i-7])+\" and \"+ str(df.columns[j-7])\n",
    "            plt.figure(figsize=(100, 50))\n",
    "        else:\n",
    "            continue\n",
    "        scores=[]\n",
    "        clusters=[]\n",
    "        parameters=[]\n",
    "        linkage_array=[\"average\",\"single\"]\n",
    "        affinity_array=[\"euclidean\",\"manhattan\",\"cosine\"]\n",
    "        z=1\n",
    "        for k in range(1,3):\n",
    "            link=linkage_array[k-1]\n",
    "            for l in range (1,4):\n",
    "                a=affinity_array[l-1]\n",
    "                if link==\"average\" and a==\"cosine\":\n",
    "                    start=0.3\n",
    "                    stop=0.8\n",
    "                    step=0.1\n",
    "                elif link==\"single\" and a==\"cosine\":\n",
    "                    start=0.05\n",
    "                    stop=0.1\n",
    "                    step=0.01            \n",
    "                elif link==\"single\" and (a==\"manhattan\" or a==\"euclidean\"):\n",
    "                    start=0.05\n",
    "                    stop=0.1\n",
    "                    step=0.01\n",
    "                else:\n",
    "                    start=1\n",
    "                    stop=6\n",
    "                    step=1\n",
    "                for m in np.arange (start,stop,step):\n",
    "                    d=m\n",
    "                    clt = AgglomerativeClustering(linkage=link, affinity=a,distance_threshold=d,n_clusters=None)\n",
    "                    model = clt.fit(df[[df.columns[i],df.columns[j]]])\n",
    "                    if (len(np.unique(model.labels_)))>1:\n",
    "                        plt.subplot(13, 5,z)\n",
    "                        plot_dendrogram(model, truncate_mode=\"level\", p=8)\n",
    "                        plt.title(str(link)+\" , \"+str(a)+\" , \"+str(d)+\" for \"+ Features_string)\n",
    "                        plt.subplot(13, 5,z+1)\n",
    "                        plt.scatter(x=df[df.columns[i-7]],y=df[df.columns[j-7]],c=model.labels_)\n",
    "                        plt.title(str(link)+\" , \"+str(a)+\" , \"+str(d)+\" for \"+ Features_string)\n",
    "                        scores.append(silhouette_score(df[[df.columns[i],df.columns[j]]],model.labels_ ))\n",
    "                        clusters.append(len(np.unique(model.labels_)))\n",
    "                        parameters.append(str(link)+\" , \"+str(a)+\" , \"+str(d)+\" for \"+ Features_string)\n",
    "                        z=z+2\n",
    "        plt.subplot(13,5,z)\n",
    "        plt.plot(clusters, scores, marker='o')\n",
    "        plt.title('Silhouette method')\n",
    "        plt.xlabel('Number of clusters')\n",
    "        plt.ylabel('Score')\n",
    "        print( \"number of clusters= \" + str(clusters[scores.index(max(scores))])+\n",
    "              \" and parameters \"+str(parameters[scores.index(max(scores))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "clusters=[]\n",
    "parameters=[]\n",
    "EPSS=[]\n",
    "Min_Sampless=[]\n",
    "for i in range(7,14):\n",
    "    for j in range(7,14):\n",
    "        if (i!=j):\n",
    "            Features_string=str(df.columns[i-7])+\" and \"+ str(df.columns[j-7])\n",
    "            plt.figure(figsize=(100, 50))\n",
    "        else:\n",
    "            continue\n",
    "        scores=[]\n",
    "        clusters=[]\n",
    "        parameters=[]\n",
    "        EPSS=[]\n",
    "        Min_Sampless=[]\n",
    "        z=1\n",
    "        for Min_Samples in range (5,26):\n",
    "            for EPS in np.arange (0.1,4,0.3):\n",
    "                model = (DBSCAN(eps=EPS, min_samples=Min_Samples)).fit(df[[df.columns[i],df.columns[j]]])\n",
    "                if (len(np.unique(model.labels_)))>1:\n",
    "                    plt.subplot(21, 10,z)\n",
    "                    plt.scatter(x=df[df.columns[i-7]],y=df[df.columns[j-7]],c=model.labels_)\n",
    "                    plt.title(str(EPS)+\" , \"+str(Min_Samples)+\" for \"+ Features_string)\n",
    "                    scores.append(silhouette_score(df[[df.columns[i],df.columns[j]]],model.labels_ ))\n",
    "                    clusters.append(len(np.unique(model.labels_)))\n",
    "                    EPSS.append(EPS)\n",
    "                    Min_Sampless.append(Min_Samples)\n",
    "                    parameters.append(str(EPS)+\" , \"+str(Min_Samples)+\" for \"+ Features_string)\n",
    "                    z=z+1\n",
    "        plt.subplot(21,10,z)\n",
    "        plt.plot(EPSS, scores, marker='o')\n",
    "        plt.title(\"EPS Variation with Silhouette Score \")\n",
    "        plt.xlabel('EPS')\n",
    "        plt.ylabel('Score')\n",
    "        plt.subplot(21,10,z+1)\n",
    "        plt.plot(Min_Sampless, scores, marker='o')\n",
    "        plt.title(\"Min_points Variation with Silhouette Score \")\n",
    "        plt.xlabel('Min_points')\n",
    "        plt.ylabel('Score')\n",
    "        print( \"number of clusters= \" + str(clusters[scores.index(max(scores))])+\n",
    "              \" and parameters \"+str(parameters[scores.index(max(scores))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For income and Age only \n",
    "from scipy.stats import multivariate_normal\n",
    "scores=[]\n",
    "clusters=[]\n",
    "parameters=[]\n",
    "EPSS=[]\n",
    "Min_Sampless=[]\n",
    "\n",
    "Features_string=str(df.columns[9-7])+\" and \"+ str(df.columns[11-7])\n",
    "data=(df[[df.columns[9],df.columns[11]]]).to_numpy()\n",
    "plt.figure(figsize=(100, 50))\n",
    "covariance_type=[\"full\", \"tied\", \"diag\", \"spherical\"]\n",
    "for k in  range(0,4):\n",
    "    gm = GaussianMixture(n_components=2,covariance_type=covariance_type[k]).fit(data)\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = np.zeros((xx.shape[0], xx.shape[1]))\n",
    "    for l in range(gm.n_components):\n",
    "        density = multivariate_normal(mean=gm.means_[l], cov=gm.covariances_[l]).pdf(np.dstack((xx, yy)))\n",
    "        Z += density * gm.weights_[l]\n",
    "\n",
    "    # Plot the resulting contour plot\n",
    "    plt.subplot(2,2,k+1)\n",
    "    plt.contour(xx, yy, Z, levels=10, linewidths=1, colors='black')\n",
    "    plt.contourf(xx, yy, Z, levels=10, cmap='tab20b')\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=gm.predict(data), cmap='tab20b')\n",
    "    plt.title('Gaussian Mixture Clustering using %s' % covariance_type[k]+ \" for \"+Features_string)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Clustering Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
